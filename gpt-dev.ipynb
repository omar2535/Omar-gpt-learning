{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eb125ae-31a8-46e3-888a-efe513998857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "text = []\n",
    "for filename in os.listdir(\"training_material\"):\n",
    "    with open(os.path.join(\"training_material\", filename), \"r\") as f:\n",
    "        text.append(f.read())\n",
    "\n",
    "joined_text = \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129394\n"
     ]
    }
   ],
   "source": [
    "print(len(joined_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " _·⿰⿱⿳、。《》「」㿉㿗䈽䏶䐈䐜䘌䘒䘓䬪䵟一丁七丈三上下不且世丙丞並中丰串丸丹主乃久之乍乎乏乖乘乙九也乳乾亂了予事二云互五井亟亡亢交亦亨京亭亮人仁仆今介仍仕他付仙代令以仰仲任伍伏伐休伯伸伺似佃但佇位低住佐佑何佗余佛作佩佳併使來例供依侯侵便促俗保俞信修俯俱倉個倍倒倘候倚借倦倫值假偉偏偑偕停健側偶偽備傢催傳傷傾僅僕僚僧僭僮僵價僻儒優兀允元兄充先光克免兒兔兜入內全兩八公六兮共其具典兼再冒冠冬冰冶冷冽准凋凌凍凝凡凰凹出刀分切刈刑列初別利刪刮到制刷刺刻則剉削剋前剔剖剛剜剝割劇劈劉劑力功加劣助努劫勁勉勒勔動勝勞勢勺勻勾勿包化北匙匪匯匱十千升午半卑卒協南博卜占危即卵卷卻卿厚原厥厭厲去參又叉及友反叔取受叢口古句只叫可台叱史右司吃各吅合吉同名后吏吐向君吞吟否含吮吳吸吹吼吾呃呆告周味呵呷呻呼命咀咂和咎咒咨咬咯咳咸咽哀品哉哮哺哽唇唐唧唯唾問啖啜啞啟啼啾善喉喎喘喙喚喜喝喪單喻嗄嗅嗇嗌嗜嗣嗽嘆嘈嘉嘔嘗嘯嘴噀噁噎噓噙噤噦器噫噯噴嚏嚥嚨嚮嚴嚼囊囚四回因困固國圍園圓圖團土在圭地均坐坑坡坤坦坩坼垂垢垣埋城域執培堂堅堤堪塊塌塗塞填塵境墜增墟墨墮壁壅壑壓壙壞士壬壯壺壽夏夕外多夜夢大天太夫夭央失夷夾奄奇奈奉奏奔套奠奧奪奭女奴奸奼好如妃妄妊妒妙妝妨妻始姑姜威娠娥娶婆婉婢婦媒媚媾嫌嫩嬌嬰子孔孕字存孟孤孩孫學宅守安宋完宏宗官定宛宜客宣室宦宮害家容宿寄密寇富寐寒寓察寡寢實寧審寫寬寰寵寶寸寺封射將專尉尊尋對導小少尖尚尤就尺尾尿局屁居屈屋屍屎屏屑展屢履屬山岐岩岸峭峰峻崇崔崖崩嵇嵐嵩嶺巔川州巡工左巧巨差己已巳巴巷巽市布希帖帚帛帝帥師席帶常幣干平年幸幹幼幽幾床序底庚府度座庭庵庶康庸廉廓廟廢廣廩廬廳延廷建廿弊式弓引弗弘弛弟弦弱張強彈彌彎彘形彥彰影役彼往待後徐徑徒得從御復循微徵德徹徽心必忌忍志忘忠忡忤快念忽忿怒怔思怠急性怪怯恃恍恐恙恚恣恭息悅悉悍悟患悲悵悶悸情惆惋惑惚惜惟惠惡惱想愁愈意愚愛感慄慈慎慓慘慢慣慧慨慮慶慾憂憊憎憒憨憫憶憹應懊懣懦懶懷懸懼懿戊戎成我戒或戟截戰戴戶房所扁扇手才扎打托扣扦扶承技把抑投折抱抵抹抽拈拌拒拔拘拙招拜拭拯拱拳拷拾持指按挑挺挼挾捉捕捶捷捻掃授掉掌掐排掘掛掠採探掣接控推措揀揉提插揚換握揩揭揮援損搏搐搓搖搗搜搦搽摘摩摻撥撩撫撬撮撲撼擁擂擇擊擎擒擔擘據擦擯擴擾攘攙攝攢攣攤攪支收改攻放故效敏救敕敗教敢散敵敷數斂斃斆文斌斑斕斗料斛斜斟斤斥斧斬斯新斷方於施旁旃旅旋族既日旦旨早旬旱旺昂昌明昏易昔星春昧是時晉晚晝晡晨普景晶智暈暍暑暖暗暢暫暮暴曆曉曖曝曬曰曲更曷書曾最會月有服望朝期木未末本朮朱朴朵朽杆杉李杏材村杖杜杞束杭東杵杷松板枇枉析枕林枚果枝枯枲枳架枸柄柏某染柔查柱柳柴柿栗栝栲栳核根格桂桃桐桑桔桵桶梁梅梔梗條梢梧梨梳棄棕棗棘棠棧棲棺椇植椒椹椿楂楊楓楔楚楝楞楫楮極概榆榔榛榧榨榮榴槁槌槐槽槿樂樓樗標樞樟樸樹橄橘機橫檀檐檢檳檽櫚櫛櫻權欖次欣欲款歇歌歐歙歟歡止正此步武歧歲歷歸死殃殆殊殞殭段殺殼殿毀毋母每毒比毛毫氏民氣水永汁求汗汙汝汞江池汪汲決沃沈沉沌沏沐沒沖沙沫河沸油治沾況泄泉泊泔法泛泡波泣泥注泰洒洗洛洞津洪洵活洽流浙浣浮浴海浸消涉涎涓涕液涸涼淋淘淚淡淨淫淬淮深淵混淺添清渙減渠渣測渴游渾湊湖湧湯溉溏源準溝溢溪溫溲溶溺溼滄滅滇滋滌滑滓滬滯滲滴滷滾滿漁漂漆漉漏漓漚漢漫漬漱漲漸漿潔潛潞潤潮潰澀澄澡澤澼激濁濃濕濟濡濾濿瀉瀛瀝灌灑火灰灶灸灼炎炒炙炭炮炷為烈烏烘焉焊焙焚無焦焮焰然煅煉煎煙煤照煨煩煮煽熊熏熙熟熨熬熱熾燃燈燎燒燕營燥燭燻爍爛爨爪爭父爾牆片版牙牛牝牟牡牢牧物特牽牾犀犬犯狀狂狐狗狾猗猛猝猶獄獅獨獲獵獸獺玄率玉王玲珀珍珠現球理琥琰琳瑙瑞瑣瑩瑪環瓏瓜瓢瓣瓦瓶甄甑甕甘甚甜生產甥用甫田由甲男界畏留畜畢略番異當畹疊疏疑疔疙疝疣疥疫疰疱疳疴疵疸疹疼疽疾痁痂痃痄病症痊痎痔痕痘痙痛痞痠痢痣痧痰痱痹痼痿瘀瘋瘍瘑瘓瘕瘙瘛瘟瘡瘢瘤瘥瘦瘧瘩瘰瘲瘳瘴瘵瘻療癃癆癇癉癌癔癖癘癜癡癢癤癥癧癩癬癭癮癰癱癲癸登發白百皂的皆皇皋皮皯皰皴皶皺盂盆益盛盜盞盡監盤盥盪目盲直相省眉看真眠眥眩眵眼眾睛睡督睪睫睹睽瞎瞑瞪瞳瞻瞿矛矢矣知短矯石砂砉砍砒研砧破硃硇硝硫硬硼碎碗碧碩確碾磁磚磨磺礙礞礦礬社祐祖祗祛神祟祥禁禍禦禮禱禽禾禿秀秋科秘秦秧秫移稀程稍稜稟稠種稱稷稻稽穀穄積穎穗穢穩穴究空穿突窒窗窠窩窮窺竄竅竊立竟章童竭端竹笑符第筆等筋筌筍筒答策箍箔箕管箭箱節範篆築篋篤篦篩簇簸籠米粉粒粕粗粟粥粱粳粹粽精糊糖糙糜糝糞糟糠糧糯糰系約紅紉紋納紐純紙級紛紜紝素索紫紮累細終結絕絞絡絮統絲絳絹經綠綬綱網綴綸綿緄緊線緣編緩緯縈縊縛縣縫縮縱縶總績繁繆繩繫繭繰繼纂續纏缸缺缽罈罌罐罔罕罨罩罪置罯罰署罷羅羊羌美羔羖羚羜羝群義羯羸羹羽翁翅翎習翔翣翱翳翹翻翼老考者耆而耐耒耗耳耶耽聊聖聚聞聤聰聲職聽聾肅肆肉肋肌肘肚肛肝股肢肥肪肫肭肯肱育肷肺胃背胎胕胞胡胬胭胱胵胸能脂脅脆脈脊脘脛脫脬脹脾腋腎腐腑腓腕腠腥腦腫腮腰腳腴腸腹腿膀膃膈膊膏膚膜膝膠膨膩膹膻膽膿臁臂臆臉臊臌臍臘臚臞臟臣臥臨自臬臭至致臺臻臼臾舂舅與興舉舊舌舍舐舒舟舶船良艱色艽艾芋芍芎芒芙芝芡芤芥芨芩芫芬芯芰花芳芷芽苑苓苔苗苟苡苣若苦苧苨英茄茅茇茈茗茯茱茴茵茶茸茹茺荄草荊荑荒荔荷荽莊莎莖莝莢莧莪莫莽菀菁菇菊菌菔菖菘菜菝菟菩華菱菴菽萄萆萊萍萎萬萱萵萸落葉葎著葙葛葜葡董葦葫葳葵葶葷蒂蒙蒜蒡蒲蒸蒺蒼蒿蓄蓉蓋蓐蓖蓬蓮蓯蓼蓽蔓蔔蔗蔚蔞蔡蔤蔥蔬蔭蔻蕃蕈蕊蕌蕎蕒蕓蕙蕤蕨蕩蕪蕭蕷薄薇薊薏薑薔薛薟薢薤薩薪薯薰薷薹薺藁藉藏藕藜藤藥藪藭藶藻藿蘄蘅蘆蘇蘊蘚蘞蘭蘸蘿虎虐處虛虞號虧虫虻蚊蚌蚓蚣蚧蚯蚰蚱蚶蚺蛀蛆蛇蛋蛔蛛蛤蛭蛸蛻蛾蜀蜂蜆蜈蜊蜍蜒蜘蜜蜞蜣蜱蝕蝙蝠蝥蝦蝨蝸螂螣螫螯螳螵螺螻蟆蟣蟥蟬蟯蟲蟹蟻蟾蠅蠍蠟蠡蠣蠱蠲蠶蠹血衄行衍術街衛衝衡衣表衰袋被裂裕補裝裡裨裳裹製複褐褚褥襪襲西要覆見規視親覺覽觀角解觸言訂計訖記訟訣訥訪設許訶診訾詒評詞詠詢詣試詩詳詵誅誇誕語誠誡誣誤誦說誰調諄談請論諭諸諺謁謂謗謙謝謨謬證譏識譫譬議護譽讀變谷豁豆豈豉豐豔豕豚象豨豫豬豶貌貓貘貝貞貧貨貪貫責貯貴買貼貽賀賁資賈賊賜賢賣賤賦質賴購贅贊贗赤赭走起趁越趙趨足趾跌距跟路跳踏踐踞蹄蹶躁躄身車軍軟軫較載輒輔輕輩輪輯輸轂轉辛辟辣辨辭辰迅迎近返迥迨迫述迴迷迸追退送逃逆逍透逐途通逝速造連進逼逾遁遂遇遊運遍過遏道達違遙遠遣適遭遲遷遺遼遽避邁還邈邊那邪邵郁郇郡部郭郵都鄉鄒鄙鄭酌配酒酢酥酪酷酸醃醇醉醋醍醐醒醜醫醬醴醺釀釅釋里重野量釐金釘釜針釣釵鈍鈴鉛鉤銀銅銖銚銳銼鋒鋪錄錘錢錦錫錯鍋鍚鍾鎖鎮鎰鏃鏡鏽鐘鐵鐸鑑鑪鑱鑽鑿長門閃閉開閒間閔閟閣閩閭闊闌闔闕關闢阜防阻阿陀附陌降限陝除陰陳陵陶陷陸陽隆階隔際障隧隨險隱隴隸雀雁雄集雋雌雍雕雖雙雛雜雞離難雨雪雲雷需霄震霍霜霞霧露霹靂靈青靖靛靜非面靴鞋鞍鞕鞠韋韌韓韙韭音響頂頃項順須頌頏預頑頓頗領頭頰頸頹頻顆額顏願顙顛類顯顰風飄飛食飢飧飪飯飲飴飼飽餅養餌餐餒餘餚館餳饅饌首香馥馬馳駙駛駝駭駱騫騰騷驅驍驗驚驟驢骨髓體高髭髮髯髻鬃鬆鬚鬣鬥鬱鬼鬾魂魃魄魅魏魘魚魯鮓鮫鮮鯁鯉鯗鯽鰂鰻鰾鱉鱔鱗鱠鱧鱺鳥鳧鳩鳳鳴鴟鴠鴣鴨鵜鵝鶘鶡鶴鷓鷥鷹鷺鸛鹵鹹鹼鹽鹿麋麗麝麥麩麯麴麵麻黃黍黎黏黑黔默黛黜點黴黶鼎鼓鼠鼻齁齄齆齊齋齒齡齦齧齲齼龍龔龜龠！（），：；？𢘅𤸷\n",
      "2896\n"
     ]
    }
   ],
   "source": [
    "# Get unique characters that occur in the dataset\n",
    "chars = sorted(list(set(joined_text)))  # set() removes duplicates\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1480, 2078]\n",
      "甘草\n"
     ]
    }
   ],
   "source": [
    "# Create a ampping from characters to integers\n",
    "stoi = { char:index for index, char in enumerate(chars) }\n",
    "itos = { index:char for index, char in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]  # convert string to list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l])  # convert list of integers to string\n",
    "\n",
    "print(encode(\"甘草\"))\n",
    "print(decode(encode(\"甘草\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([129394]) torch.int64\n",
      "tensor([1355, 1925, 2890,  990, 1615,    0,    0, 1480, 2060, 2843,  658,    8,\n",
      "         180, 1925, 1819, 2890, 2648, 2741, 1400,    8, 1231,   24,  212, 1615,\n",
      "        1520, 2890,  941, 1031, 1217,  990,   45,  318,    8,  292, 1022, 2891,\n",
      "        1606, 1961, 1778, 2888, 1639,  533, 1681,  174,  613, 1961,    8,  981,\n",
      "        2106, 1022, 2891,  990, 1615, 1481,  973, 2860, 2396, 2890,  110, 1064,\n",
      "        1231, 2727, 2741, 2889,    8,    0,    0, 1435,  506, 2767, 2457, 1217,\n",
      "         990,    8,  902, 1662, 1386,    8,  773,  481, 2855,   75,    8,    1,\n",
      "           2, 1775, 2890, 1660,   53,    2,    0,    0, 2308, 1959, 1283, 1936,\n",
      "           0,    0, 1480, 1269])\n"
     ]
    }
   ],
   "source": [
    "# Let's encode the entire dataset, then store in a torch.Tensor\n",
    "\n",
    "import torch\n",
    "data = torch.tensor(encode(joined_text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1355, 1925, 2890,  990, 1615,    0,    0, 1480, 2060])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: tensor([1355]), target: 1925\n",
      "Context: tensor([1355, 1925]), target: 2890\n",
      "Context: tensor([1355, 1925, 2890]), target: 990\n",
      "Context: tensor([1355, 1925, 2890,  990]), target: 1615\n",
      "Context: tensor([1355, 1925, 2890,  990, 1615]), target: 0\n",
      "Context: tensor([1355, 1925, 2890,  990, 1615,    0]), target: 0\n",
      "Context: tensor([1355, 1925, 2890,  990, 1615,    0,    0]), target: 1480\n",
      "Context: tensor([1355, 1925, 2890,  990, 1615,    0,    0, 1480]), target: 2060\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"Context: {context}, target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch the predicted output\n",
    "Batch the predicted output from the tensor so that we get a xb that is the context and xy is the predicted output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 479,  271, 1534, 1811, 2890, 1950,  452,  506],\n",
      "        [1350, 2890, 1204, 2669, 1900,  949, 2890,  338],\n",
      "        [2002, 2006,   31,  573,    8, 1963, 1898, 2890],\n",
      "        [1204,  243, 2650, 1598, 2890, 1264,  390,  243]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 271, 1534, 1811, 2890, 1950,  452,  506, 1648],\n",
      "        [2890, 1204, 2669, 1900,  949, 2890,  338, 2178],\n",
      "        [2006,   31,  573,    8, 1963, 1898, 2890, 1977],\n",
      "        [ 243, 2650, 1598, 2890, 1264,  390,  243, 1645]])\n",
      "----\n",
      "when input is [479] the target: 271\n",
      "when input is [479, 271] the target: 1534\n",
      "when input is [479, 271, 1534] the target: 1811\n",
      "when input is [479, 271, 1534, 1811] the target: 2890\n",
      "when input is [479, 271, 1534, 1811, 2890] the target: 1950\n",
      "when input is [479, 271, 1534, 1811, 2890, 1950] the target: 452\n",
      "when input is [479, 271, 1534, 1811, 2890, 1950, 452] the target: 506\n",
      "when input is [479, 271, 1534, 1811, 2890, 1950, 452, 506] the target: 1648\n",
      "when input is [1350] the target: 2890\n",
      "when input is [1350, 2890] the target: 1204\n",
      "when input is [1350, 2890, 1204] the target: 2669\n",
      "when input is [1350, 2890, 1204, 2669] the target: 1900\n",
      "when input is [1350, 2890, 1204, 2669, 1900] the target: 949\n",
      "when input is [1350, 2890, 1204, 2669, 1900, 949] the target: 2890\n",
      "when input is [1350, 2890, 1204, 2669, 1900, 949, 2890] the target: 338\n",
      "when input is [1350, 2890, 1204, 2669, 1900, 949, 2890, 338] the target: 2178\n",
      "when input is [2002] the target: 2006\n",
      "when input is [2002, 2006] the target: 31\n",
      "when input is [2002, 2006, 31] the target: 573\n",
      "when input is [2002, 2006, 31, 573] the target: 8\n",
      "when input is [2002, 2006, 31, 573, 8] the target: 1963\n",
      "when input is [2002, 2006, 31, 573, 8, 1963] the target: 1898\n",
      "when input is [2002, 2006, 31, 573, 8, 1963, 1898] the target: 2890\n",
      "when input is [2002, 2006, 31, 573, 8, 1963, 1898, 2890] the target: 1977\n",
      "when input is [1204] the target: 243\n",
      "when input is [1204, 243] the target: 2650\n",
      "when input is [1204, 243, 2650] the target: 1598\n",
      "when input is [1204, 243, 2650, 1598] the target: 2890\n",
      "when input is [1204, 243, 2650, 1598, 2890] the target: 1264\n",
      "when input is [1204, 243, 2650, 1598, 2890, 1264] the target: 390\n",
      "when input is [1204, 243, 2650, 1598, 2890, 1264, 390] the target: 243\n",
      "when input is [1204, 243, 2650, 1598, 2890, 1264, 390, 243] the target: 1645\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "# sample the batch\n",
    "# each chunk of xb is a set of the training data\n",
    "# each chunk of yb is the next word in the training data\n",
    "xb, yb = get_batch('train') \n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 479,  271, 1534, 1811, 2890, 1950,  452,  506],\n",
      "        [1350, 2890, 1204, 2669, 1900,  949, 2890,  338],\n",
      "        [2002, 2006,   31,  573,    8, 1963, 1898, 2890],\n",
      "        [1204,  243, 2650, 1598, 2890, 1264,  390,  243]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # our input to the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2896])\n",
      "tensor(8.2181, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "那布老箕菖撲擒程廉包倘洽蝨恃待論乎蒿鶡熾陳響蛛鼓口準改腦劣坦內闕戴互直狾詣肭衄騫南宮坦非啼見拱土逐鈴浴蚺荽足揉欣令腫虧敗至六蠟罕貘駱糯鶡嚥徵篆媚釐戎駛旨江麥雜百盂佃醜遊忽幸鴟醴麝料癜偑咸拭拒蓉興陶枇百\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)  - unscaled score for each token\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
